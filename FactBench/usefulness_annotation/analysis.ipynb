{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['conversation_hash', 'user_prompts', 'model_responses', 'embedding',\n",
       "        'embedding_dim10_Nnbr15', 'Topics', 'model', 'factual_vs_faithful',\n",
       "        'scores'],\n",
       "       dtype='object'),\n",
       " Index(['conversation_hash', 'user_prompts', 'model_responses', 'embedding',\n",
       "        'embedding_dim10_Nnbr15', 'Topics', 'model', 'factual_vs_faithful',\n",
       "        'scores'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpt_4_annotation = pd.read_csv(\"../data/lmsys_data/lmsys_user_prompts_jaccard0.9_w_factually_w_topics_w_model_v1.1_prompts_evaluated_gpt4.csv\")\n",
    "llama_3_annotations = pd.read_csv(\"../data/lmsys_data/lmsys_user_prompts_jaccard0.9_w_factually_w_topics_w_model_v1.1_prompts_evaluated_llama3_instruct.csv\")\n",
    "# df = pd.read_csv(\"final_dataset_w_avg_scors.csv\")\n",
    "gpt_4_annotation.columns, llama_3_annotations.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "pattern = r'(\\{.*\"Clarity\".*\"Generalizability\".*\"Relevance\".*\"Actionability\".*\"Feasibility\".*\\})'\n",
    "def extract_valid_json(val):\n",
    "    if isinstance(val, str):\n",
    "        match = re.search(pattern, val, re.DOTALL)  # Extract the dictionary portion\n",
    "        if match:\n",
    "            try:\n",
    "                # Clean up the extracted match by removing newlines and unnecessary characters\n",
    "                json_str = match.group(1).replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n",
    "                return ast.literal_eval(json_str)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# extract_valid_json(x)\n",
    "# gpt_4_annotation['scores_gpt4'] = gpt_4_annotation['scores'].apply(extract_valid_json)\n",
    "# gpt_4_annotation['scores_llama3'] = llama_3_annotations['scores'].apply(extract_valid_json)\n",
    "\n",
    "# non_matching_rows_llama3 = gpt_4_annotation[gpt_4_annotation['scores_llama3'].isnull() & llama_3_annotations['scores'].notnull()]\n",
    "# print(len(non_matching_rows_llama3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame({'user_prompts': gpt_4_annotation['user_prompts'],\n",
    "                        'scores_gpt4': gpt_4_annotation['scores_gpt4'],\n",
    "                        'scores_llama3': gpt_4_annotation['scores_llama3']})\n",
    "\n",
    "df_scores = df_scores.groupby('user_prompts').first().reset_index()\n",
    "# df_scores_unique.head(5)\n",
    "\n",
    "# df_scores['GPT-4 Scores'] = df_scores['GPT-4 Scores'].apply(extract_valid_json)\n",
    "# df_scores['LLAMA-3 Scores'] = df_scores['LLAMA-3 Scores'].apply(extract_valid_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_eval_scores(row):\n",
    "    overall_score = 0\n",
    "    for k in ['Clarity', 'Generalizability', 'Relevance', 'Feasibility']: #actionability has been removed from the evaluation\n",
    "        if row['scores_gpt4'] is None and row['scores_llama3'] is None:\n",
    "            overall_score += 0\n",
    "        elif row['scores_llama3'] is None:\n",
    "            overall_score += row['scores_gpt4'][k]\n",
    "        elif row['scores_gpt4'] is None:\n",
    "            overall_score += row['scores_llama3'][k]\n",
    "        else:\n",
    "            overall_score += (row['scores_gpt4'][k] + row['scores_llama3'][k])/2\n",
    "    return overall_score/4\n",
    "df_scores['prompt_score'] = df_scores.apply(combine_eval_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_prompts', 'scores_gpt4', 'scores_llama3', 'prompt_score'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores.columns\n",
    "# df_scores.to_csv(\"../data/lmsys_data/lmsys_user_prompts_evaluated_avg_scores_comb.csv\", index=False)\n",
    "# df_unique_user_prompts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics\n",
      "168    4.889474\n",
      "365    4.800000\n",
      "285    4.700000\n",
      "91     4.700000\n",
      "266    4.700000\n",
      "         ...   \n",
      "186    2.786667\n",
      "377    2.750000\n",
      "310    2.700000\n",
      "167    2.000000\n",
      "324    1.875000\n",
      "Name: comb_avg_score, Length: 298, dtype: float64\n",
      "20824 5610\n"
     ]
    }
   ],
   "source": [
    "sorted_df = df.groupby('Topics')['comb_avg_score'].mean().sort_values(ascending=False)\n",
    "print(sorted_df)\n",
    "print(len(df), len(df[(df['comb_avg_score'] >= 4) & (df['Topics'] != -1)]))\n",
    "\n",
    "# index_of_topic_minus_1 = sorted_df.index.tolist().index(-1)\n",
    "# print(index_of_topic_minus_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2688"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['comb_avg_score'] >= 4.0) & (df['Topics'] != -1) & (df['model'].isin(['gpt-4', 'gpt-3.5-turbo', 'claude-2', 'palm-2', 'claude-1', 'claude-instant-1']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"../data/lmsys_data/top_4000_data_instances.csv\"\n",
    "# input_file_path = \"../data/lmsys_data/top_3000_data_noise_excluded.csv\"\n",
    "df_final = pd.read_csv(input_file_path)\n",
    "\n",
    "# df_complete = pd.read_parquet(input_file_path)\n",
    "\n",
    "# df_s = df_complete[df_complete['user_prompts'] == \"what kind of poem rhymes on every even numbered line?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_final[df_final[\"Topics\"] != -1])\n",
    "len(df_final.groupby('Topics').first())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
